x-gpu: &gpu
  runtime: nvidia
  deploy:
    resources:
      reservations:
        devices:
          - capabilities: [gpu]
  environment:
    - NVIDIA_VISIBLE_DEVICES=all
    - NVIDIA_DRIVER_CAPABILITIES=all

services:
  ollama:
    <<: *gpu
    image: ollama/ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_NEW_ENGINE=${OLLAMA_NEW_ENGINE:-false}
    volumes:
      - ./data/ollama:/root/.ollama

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    profiles:
      - all
      - openwebui
    ports:
      - "3000:8080"
    volumes:
      - ./data/open-webui:/app/backend/data
    environment:
      # https://docs.openwebui.com/getting-started/env-configuration
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - COMFYUI_BASE_URL=http://comfyui:8188
      - AUTOMATIC1111_BASE_URL=http://automatic1111:7860
      - ENABLE_IMAGE_GENERATION=true
      - IMAGE_GENERATION_ENGINE=comfyui
      - IMAGE_GENERATION_MODEL=v1-5-pruned-emaonly-fp16.safetensors
      - WEBUI_AUTH=${WEBUI_AUTH:-false}
    extra_hosts:
      - host.docker.internal:host-gateway
    depends_on:
      - ollama
  
  anythingllm:
    image: mintplexlabs/anythingllm
    restart: unless-stopped
    profiles:
      - all
      - anythingllm
    ports:
      - "3001:3001"
    # https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/.env.example
    environment:
      - STORAGE_DIR=/app/server/storage
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_PATH=http://ollama:11434
      - VECTOR_DB=lancedb
      - OLLAMA_MODEL_PREF=llama3.1:8b
      #- OLLAMA_MODEL_TOKEN_LIMIT=4096
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_BASE_PATH=http://ollama:11434
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
      - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192
    volumes:
      - ./data/anythingllm/storage:/app/server/storage
      - ./data/anythingllm/collector/hotdir:/app/collector/hotdir
      - ./data/anythingllm/collector/output:/app/collector/output
    depends_on:
      - ollama

  comfyui:
    <<: *gpu
    build:
      context: .
      dockerfile: Dockerfile
      target: comfyui
    profiles:
      - all
      - comfyui
    ports:
      - "8188:8188"
    volumes:
      - ./data/comfyui:/data
      - ./data/huggingface:/huggingface
      - ./data/torch.hub:/torch.hub

  automatic1111:
    <<: *gpu
    build:
      context: .
      dockerfile: Dockerfile
      target: automatic1111
    profiles:
      - all
      - automatic1111
    ports:
      - "7860:7860"
    volumes:
      - ./data/automatic1111:/data
      - ./data/huggingface:/huggingface
      - ./data/torch.hub:/torch.hub

  tika:
    image: apache/tika:latest-full
    ports:
      - "9998:9998"
    restart: unless-stopped
    profiles:
      - all
      - tika
    environment:
      - JAVA_OPTS=-Xmx2g
    configs:
      - source: tika_config
        target: /tika-config.xml
    command: [
      "tika-server",
      "--config=/tika-config.xml",
      "--port=9998"
    ]

  jupyter:
    # https://jupyter-docker-stacks.readthedocs.io/en/latest/using/recipes.html
    <<: *gpu
    #image: jupyter/datascience-notebook
    image: quay.io/jupyter/pytorch-notebook:x86_64-cuda12-latest
    ports:
      - "8888:8888"
    profiles:
      - all
      - jupyter
    environment:
      - GRANT_SUDO=yes
      - HF_HOME=/huggingface
      - TORCH_HOME=/torch.hub
    command: start-notebook.py --IdentityProvider.token='' --notebook-dir=/notebooks
    volumes:
      - ./data/jupyter:/notebooks
      - ./data/huggingface:/huggingface
      - ./data/torch.hub:/torch.hub

  stack-overview:
    build: stack-overview
    profiles:
      - all
    ports:
      - "8500:8500"
    volumes:
      - ./docker-compose.yml:/app/docker-compose.yml:ro

configs:
  tika_config:
    content: |
      <?xml version="1.0" encoding="UTF-8"?>
      <properties>
        <parsers>
          <parser class="org.apache.tika.parser.DefaultParser"/>
        </parsers>
        <ocr>
          <tesseractPath>/usr/bin/tesseract</tesseractPath>
          <language>deu+eng</language>
          <enableImageProcessing>true</enableImageProcessing>
          <ocrStrategy>auto</ocrStrategy>
        </ocr>
      </properties>