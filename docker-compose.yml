services:
  ollama:
    image: ollama/ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - OLLAMA_NEW_ENGINE=${OLLAMA_NEW_ENGINE:-false}
    volumes:
      - ./data/ollama:/root/.ollama

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    profiles:
      - openwebui
      - all
    ports:
      - "3000:8080"
    volumes:
      - ./data/open-webui:/app/backend/data
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - COMFYUI_BASE_URL=http://comfyui:8188
      - AUTOMATIC1111_BASE_URL=http://automatic1111:7860
      - ENABLE_IMAGE_GENERATION=true
      - WEBUI_AUTH=${WEBUI_AUTH:-true}
    extra_hosts:
      - host.docker.internal:host-gateway
    depends_on:
      - ollama
  anythingllm:
    image: mintplexlabs/anythingllm
    restart: unless-stopped
    profiles:
      - anythingllm
      - all
    ports:
      - "3001:3001"
    # https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/.env.example
    environment:
      - STORAGE_DIR=/app/server/storage
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_PATH=http://ollama:11434
      - VECTOR_DB=lancedb
      - OLLAMA_MODEL_PREF=llama3.1:8b
      #- OLLAMA_MODEL_TOKEN_LIMIT=4096
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_BASE_PATH=http://ollama:11434
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
      - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192
    volumes:
      - ./data/anythingllm/storage:/app/server/storage
      - ./data/anythingllm/collector/hotdir:/app/collector/hotdir
      - ./data/anythingllm/collector/output:/app/collector/output
    depends_on:
      - ollama

  comfyui:
    build:
      context: .
      dockerfile: Dockerfile
      target: comfyui
    profiles:
      - comfyui
      - all
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    ports:
      - "8188:8188"
    volumes:
      - ./data/comfyui:/data
      - ./data/huggingface:/huggingface

  automatic1111:
    build:
      context: .
      dockerfile: Dockerfile
      target: automatic1111
    profiles:
      - automatic1111
      - all
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    ports:
      - "7860:7860"
    volumes:
      - ./data/automatic1111:/data
      - ./data/huggingface:/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  tika:
    image: apache/tika:latest-full
    ports:
      - "9998:9998"
    restart: unless-stopped
    environment:
      - JAVA_OPTS=-Xmx2g
    configs:
      - source: tika_config
        target: /tika-config.xml
    command: [
      "tika-server",
      "--config=/tika-config.xml",
      "--port=9998"
    ]

configs:
  tika_config:
    content: |
      <?xml version="1.0" encoding="UTF-8"?>
      <properties>
        <parsers>
          <parser class="org.apache.tika.parser.DefaultParser"/>
        </parsers>
        <ocr>
          <tesseractPath>/usr/bin/tesseract</tesseractPath>
          <language>deu+eng</language>
          <enableImageProcessing>true</enableImageProcessing>
          <ocrStrategy>auto</ocrStrategy>
        </ocr>
      </properties>