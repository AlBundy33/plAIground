services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./data/ollama:/root/.ollama

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - ./data/open-webui:/app/backend/data
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - COMFYUI_BASE_URL=http://comfyui:8188
      - AUTOMATIC1111_BASE_URL=http://automatic1111:7860
      - ENABLE_IMAGE_GENERATION=true
    extra_hosts:
      - host.docker.internal:host-gateway
    depends_on:
      - ollama

  comfyui:
    build:
      context: .
      dockerfile: Dockerfile
      target: comfyui
    profiles:
      - tbd
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    ports:
      - "8188:8188"
    volumes:
      - ./data/comfyui:/workspace
      - ./data/huggingface:/huggingface

  automatic1111:
    build:
      context: .
      dockerfile: Dockerfile
      target: automatic1111
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    ports:
      - "7860:7860"
    volumes:
      - ./data/automatic1111:/data
      - ./data/huggingface:/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

